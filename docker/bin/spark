#!/bin/bash

set -x
set -e

# Configure environment
export ALLUXIO_VERSION_NUM=2.2.0
export ALLUXIO_VERSION=alluxio-2.2.0    
export SPARK_PY_VERSION=0.0.1
export SPARK_VERSION=2.4.5
export HADOOP_VERSION=3.1.3
export SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-without-hadoop
export HADOOP_HOME=$PWD/hadoop-${HADOOP_VERSION}
export APP_HOME=$PWD

######################################################################
##
##  Build a Spark-Alluxio docker image.
##    * Add configuration to incorporate Hadoop libraries.
##    * Get the appropriate AWS libraries for S3 integration.
##    * Add all of this to our Spark distribution. 
##    * Build a docker image packaging it all.
##    * Login to docker and push the image.
##
######################################################################
configure_alluxio () {
    # Configure Alluxio
    if [ ! -f jars/$ALLUXIO_VERSION-client.jar ]; then
        id=$(docker create alluxio/alluxio:$ALLUXIO_VERSION_NUM)
        docker cp $id:/opt/alluxio/client/$ALLUXIO_VERSION-client.jar $SPARK_HOME/jars/$ALLUXIO_VERSION-client.jar
        docker rm -v $id 1>/dev/null
    fi
    cp blackbalsam-jupyter/core-site.xml $SPARK_HOME/conf
}
build () {
    # Get spark, hadoop, unpack, and clean up.
    blackbalsam-jupyter/libspark get_spark clean

    # Get Alluxio jar and config files.
    configure_alluxio

    # Build Docker images
    source ~/.dockerhub
    docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD    
    cd $SPARK_HOME
    pwd
    for op in build push; do
	./bin/docker-image-tool.sh -r renciorg -t $SPARK_PY_VERSION -b spark -p $APP_HOME/spark/Dockerfile.pyspark $op
    done
}

$*

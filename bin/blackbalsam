#!/bin/bash

#############################################################
##
##  B L A C K B A L S A M
##
##  https://github.com/stevencox/blackbalsam
##
##  A multi modal data science environment featuring
##    User Experience: Jupyter Notebooks
##    Visualization: Plotly, Bokeh, Leaflet, Yellowbrick, Seaborn,...
##    Compute: CPU / GPU / Apache Spark
##    Storage: NFS / Minio & S3 / Alluxio
## 
#############################################################
set -e #x

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
BLACKBALSAM_HOME=$( dirname $DIR )

export RELEASE=blackbalsam
export NAMESPACE=$RELEASE
export JHUB_VERSION=0.8.2
export DIST=$BLACKBALSAM_HOME/bin
export ETC=$BLACKBALSAM_HOME/etc
export PATH=$DIST/darwin-amd64:$PATH
export NFS_PVC=blackbalsam-jhub-nfs-pvc
# TODO: This is OSX specific. Fix.
export helm_dist=helm-v2.16.3-darwin-amd64.tar.gz
export helm_url=https://get.helm.sh/$helm_dist

#############################################################
##
##  Utilities:
##    Make namespace: Initialize the namespace.
##    Set YAML value
##    Generate secrets
##
#############################################################
kc () {
    kubectl --namespace $NAMESPACE $*
}
make_namespace () {
    if [ "$(kubectl get namespaces | grep -c $NAMESPACE)" == 0 ]; then
        kubectl create namespace $NAMESPACE
    fi
}
yaml_set_val () {
    yaml-set \
	--change="$1" \
        --value=$2 \
        -F dquote \
        $3
}
secret () {
    secret_name=blackbalsam-secrets
    secret_config=$ETC/secrets/secrets.yaml
    query="{.items[?(@.metadata.name=='"$secret_name"')].metadata.name}"
    secret_exists=$(kc get secrets -o=jsonpath="$query" | grep -c $secret_name || true)
    up () {
	if [ "$secret_exists" -eq 0 ]; then
	    cp $secret_config.template $secret_config
	    yaml_set_val stringData.jupyterhub_secret_token $jupyterhub_secret_token $secret_config
	    yaml_set_val stringData.github_client_secret $github_client_secret $secret_config
            # Minio has standard names for these. Perhaps separate to a different secret.
	    yaml_set_val stringData.accesskey $minio_access_key $secret_config
	    yaml_set_val stringData.secretkey $minio_secret_key $secret_config
	    yaml_set_val stringData.neo4j_password $neo4j_password $secret_config
	    kc apply -f $secret_config
	fi
    }
    name ()  {
        echo $secret_name
    }
    down () {
	if [ "$secret_exists" -eq 0 ]; then
	    echo no secrets found
	else
	    kc delete -f $secret_config
	fi
    }
    status () {
	if [ "$secret_exists" -eq 0 ]; then
	    echo no secrets found
	else
	    kc get secrets/$secret_name -o=yaml
	fi
    }
    restart () {
	down
	up
    }
    $*
}
#############################################################
##
##  Manage JupyterHub.
##
#############################################################
hub () {
    conf_deprecated () {
        mkdir -p $DIST
        wget --timestamping --quiet --directory-prefix=$DIST $helm_url
        if [ ! -d $DIST/darwin-amd64 ]; then
            pushd $DIST
            tar xvzf $helm_dist
            popd
        fi
        if [ "$(kubectl get serviceaccount -n kube-system tiller -o yaml | grep -c tiller)" == 0 ]; then
            kubectl --namespace kube-system create serviceaccount tiller
            kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
            helm init --service-account tiller --history-max 100 --wait
            kubectl patch \
		    deployment \
		    tiller-deploy \
                    --namespace=kube-system --type=json \
                    --patch='[{"op": "add", "path": "/spec/template/spec/containers/0/command", "value": ["/tiller", "--listen=localhost:44134"]}]'
            helm version
            helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
            helm repo update
        fi
    }
    jupyterhub_ready_deprecated () {
        kubectl get deployments \
                --namespace=$NAMESPACE \
                --selector app=jupyterhub,component=hub \
                -o=jsonpath="{.items[?(@.metadata.labels.component=='hub')].status.readyReplicas}"
    }
    wait_deprecated () {
        while [[ ! "$(jupyterhub_ready)" == 1 ]]; do
            echo waiting
        done
    }
    up () {
	echo installing jupyterhub
        #conf
        helm version
	configure_pvc
        cp $ETC/hub/config.yaml.template config.yaml
	yaml_set_val auth.github.clientId $github_client_id config.yaml
	yaml_set_val auth.github.clientSecret $github_client_secret config.yaml
	yaml_set_val auth.github.callbackUrl $github_oauth_callback config.yaml
	yaml_set_val proxy.secretToken $jupyterhub_secret_token config.yaml
	yaml_set_val hub.baseUrl $jupyterhub_baseUrl config.yaml
	
        helm upgrade --install $RELEASE jupyterhub/jupyterhub \
             --namespace $NAMESPACE  \
             --version=$JHUB_VERSION \
             --values config.yaml
        configure_proxy
        configure_patch
    }
    configure_pvc () {
	echo check for persistent volume claim...
	pvc_exists=$(kubectl --namespace=$NAMESPACE get pvc | grep -c jhub-nfs-pvc || true)
	if [ "$pvc_exists" == 0 ]; then
	    echo install persistent volume claim...
	    kubectl --namespace=$NAMESPACE apply -f $ETC/pvc/
	fi
    }	
    configure_proxy () {
	if [ "$(kubectl get mapping | grep -c jupyter)" -eq 0 ]; then
            kubectl apply \
                    -f $ETC/ambassador/jupyterhub-ambassador-mapping.yaml \
                    --namespace=$NAMESPACE
	fi
    }
    configure_patch () {
        # https://github.com/jupyterhub/kubespawner/issues/354        
        kubectl patch deploy \
                --namespace $NAMESPACE hub \
                --type json \
                --patch '[{"op": "replace", "path": "/spec/template/spec/containers/0/command", "value": ["bash", "-c", "\nmkdir -p ~/hotfix\ncp -r /usr/local/lib/python3.6/dist-packages/kubespawner ~/hotfix\nls -R ~/hotfix\npatch ~/hotfix/kubespawner/spawner.py << EOT\n72c72\n<             key=lambda x: x.last_timestamp,\n---\n>             key=lambda x: x.last_timestamp and x.last_timestamp.timestamp() or 0.,\nEOT\n\nPYTHONPATH=$HOME/hotfix jupyterhub --config /srv/jupyterhub_config.py --upgrade-db\n"]}]'

    }
    down () {
	echo uninstalling jupyterhub
        helm version
        if [[ "$(helm ls | grep -v NAME | grep -c jupyterhub)" -gt 0 ]]; then
            echo deleting helm release $RELEASE
            helm delete $RELEASE --purge
        fi
    }
    status () {
	query="{.items[?(@.metadata.labels.app=='jupyterhub')].metadata.name}"
	kc describe pods $(kc get pods -o=jsonpath="$query")
    }
    restart () {
        down
        up
    }
    $*
}
#############################################################
##
##  Manage the software defined reverse proxy.
##
#############################################################
proxy () {
    # Install w/o CRD and change Jupyter to use service annotations, if possible.
    up () {
	echo installing ambassador...
        helm repo add datawire https://www.getambassador.io
        helm install --name ambassador --namespace $NAMESPACE --set service.loadBalancerIP=$public_ip datawire/ambassador 
    }
    down () {
	echo uninstalling ambassador...
        if [[ "$(helm ls | grep -v NAME | grep -c ambassador)" == 1 ]]; then
            helm delete ambassador --purge
        fi
        for crd in $(kubectl get customresourcedefinitions | grep ambassador | awk '{ print $1  }'); do
	    echo delelting ambassador customresourcedefinitions...
            kubectl delete customresourcedefinition $crd
        done
    }
    dashboard () {
	wget --timestamping --quiet https://metriton.datawire.io/downloads/darwin/edgectl
	chmod +x edgectl
	query="{.items[?(@.metadata.name=='ambassador')].status.loadBalancer.ingress[0].ip}"
        endpoint_ip=$(kc get svc -o=jsonpath="$query")
        ./edgectl login --namespace=$NAMESPACE $endpoint_ip
    }
    status () {
	query="{.items[?(@.metadata.labels['app\.kubernetes\.io/name']=='ambassador')].metadata.name}"
	kc describe pods $(kc get pods -o=jsonpath="$query")
    }
    restart () {
	down
	up
    }
    $*
}
#############################################################
##
##  Manage the S3 object store.
##
#############################################################
minio () {
    up () {
	echo installing minio...
        if [ "$(helm list | grep -c minio)" == 1 ]; then
            echo == WARNING ==: A minio release is already installed.
        else
            secret_name=$(secret name)
            helm install \
                 --set existingSecret=$secret_name \
		 --set persistence.existingClaim=$NFS_PVC \
		 --set persistence.subPath=minio \
                 --name minio \
                 --namespace $NAMESPACE \
                 stable/minio
        fi
    }
    down () {
	echo uninstalling minio...
        if [[ "$(helm ls | grep -v NAME | awk '{ print $1 }' | grep -c minio)" == 1 ]]; then
            helm delete --purge minio
        fi
    }
    status () {
	query="{.items[?(@.metadata.labels.app=='minio')].metadata.name}"
	kc describe pods $(kc get pods -o=jsonpath="$query")
    }
    restart () {
	down
	up
    }
    $*
}
#############################################################
##
##  Manage the memory cache data fabric.
##
#############################################################
alluxio () {
    up () {
	echo installing alluxio...
        echo "Configuring and starting alluxio..."
        export ALLUXIO_MASTER_HOST=alluxio-master-0
        export ALLUXIO_WORKER_HOST=alluxio-worker
        export MINIO_BUCKET=covid-19
        export MINIO_HOST=minio
        export MINIO_PORT=9000
        export MINIO_ACCESS_KEY=$minio_access_key
        export MINIO_SECRET_KEY=$minio_secret_key
        export alluxio_conf="./tmp/alluxio-k8s/singleMaster-localJournal/alluxio-configmap.yaml"

	echo copying alluxio config templates...
	mkdir -p tmp
	cp -r alluxio-k8s ./tmp

	envsubst < ${alluxio_conf}.template > $alluxio_conf
        echo "  configured. creating kubernetes artifacts."
        kubectl apply \
		--namespace $NAMESPACE \
		--recursive=true \
		-f ./tmp/alluxio-k8s
	rm -rf ./tmp/alluxio-k8s	
    }
    down () {
        echo "uninstalling alluxio..."
        kubectl delete \
		--ignore-not-found=true \
		--namespace $NAMESPACE \
		--recursive=true \
		-f alluxio-k8s
    }
    status () {
	query="{.items[?(@.metadata.labels.app=='alluxio')].metadata.name}"
	kc describe pods $(kc get pods -o=jsonpath="$query")
    }
    restart () {
        down
        up
    }
    $*
}
#############################################################
##
## Manage the Neo4J database.
##
#############################################################
neo4j () {
    up () {
	kubectl --namespace $NAMESPACE apply -f etc/neo4j
    }
    down () {
	kubectl --namespace $NAMESPACE delete -f etc/neo4j
    }
    status () {
	query="{.items[?(@.metadata.labels.app=='neo4j')].metadata.name}"
	kc describe pods $(kc get pods -o=jsonpath="$query")
    }
    restart () {
	down
	up
    }
    $*
}
#############################################################
##
## Manage the data corpus this cluster provides
##
#############################################################
data () {
    up () {
	# Create the updater.
	kubectl --namespace $NAMESPACE apply -f etc/data/updater
    }
    down () {
	# Delete the udpater.
	kubectl --namespace $NAMESPACE delete -f etc/data/updater
    }
    run () {
	# Run a refresh job now.
	kubectl --namespace $NAMESPACE create job --from=cronjob/update-data-set update-data
    }
    stop () {
	# Stop the running refresh job.
	kubectl --namespace $NAMESPACE delete job/update-data
    }
    status () {
	query="{.items[?(@.metadata.name=='update-data-set')].metadata.name}"
	kc describe cronjobs $(kc get pods -o=jsonpath="$query")
    }
    restart () {
	stop
	down
	up
    }
    $*
}
#############################################################
##
##  Manage the system as a whole.
##
#############################################################
SYSTEMS="secrets minio alluxio proxy neo4j hub"
up () {
    echo installing blackbalsam...
    for system in $SYSTEMS; do
	echo "  --installing $system"
        $system up
    done
}
down () {
    echo uninstalling blackbalsam...
    reversed="$(echo $SYSTEMS | awk '{ for (i=NF; i>1; i--) printf("%s ",$i); print $1; }')"
    for system in $reversed; do
        $system down
    done
    kubectl delete --ignore-not-found=true namespace $NAMESPACE
}
restart () {
    echo "restarting blackbalsam"
    echo "   bringing the system down..."
    down
    echo "   bringing the system up..."
    up
}
status () {
    for kind in deployment pod service pvc pv; do
        message=$( echo $kind | awk '{ print toupper($0) }' )
        echo $message
        kubectl get -n $NAMESPACE $kind 2>&1 | sed "s,^,   ,g"
    done
}
nodes () {
    kubectl describe nodes
}
#############################################################
##
##  Initialize.
##
#############################################################
init () {
    # Require an init file.
    if [ ! -f $HOME/.blackbalsam ]; then
        echo $HOME/.blackbalsam must exist and contain a variable called jupyterhub_secret_token
        exit 1
    fi
    source $HOME/.blackbalsam
    tiller_status=$(kubectl \
			get deployment -n kube-system \
			-o=jsonpath="{.items[?(@.metadata.labels.name=='tiller')].metadata.labels.name}" | \
			grep -c tiller)

    if [ $tiller_status -eq 1 ]; then
	return 
    fi
    echo installing tiller...
    mkdir -p $DIST

    wget --timestamping --quiet --directory-prefix=$DIST $helm_url
    if [ ! -d $DIST/darwin-amd64 ]; then
        pushd $DIST
        tar xvzf $helm_dist
        popd
    fi
    if [ "$(kubectl get serviceaccounts -n kube-system | grep -c tiller)" == 0 ]; then
        kubectl --namespace kube-system create serviceaccount tiller
    fi
    if [ "$(kubectl get clusterrolebinding | grep -c tiller)" == 0 ]; then
        kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
    fi
    helm init --service-account tiller --history-max 100 --wait
    kubectl patch deployment tiller-deploy \
            --namespace=kube-system --type=json \
            --patch='[{"op": "add", "path": "/spec/template/spec/containers/0/command", "value": ["/tiller", "--listen=localhost:44134"]}]'

    helm version
    helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
    helm repo update
}

#############################################################
##
##  Help.
##
#############################################################
help () { 
    printf "\
$0 is the command line interface for a Blackbalsam data science cluster.

Each of these commands include up, down, status, and restart.

  eg: bin/blackbalsam <command> [ up | down | status | restart ]

User Experience Services:
  hub   \tManage the JupyterHub and associated proxy mappings, storage, etc.

Storage Services:
  alluxio \tManage Alluxio services and distributed workers.
  minio \tManage Minio S3 interface. Creates a service called 'minio'.

Proxy Services:
  proxy \tManage the programmable Ambassador edge proxy.

Data Services:
  data   \tInstall the periodic data update task for this cluster.
  data run\tRun the periodic data update task now.
  data stop\tStop the running data update task if one exists

Secrets:
  secret  \tManage secrets based on environment conifguration.

General:
  up    \tExecute configurations and start all services.
  down  \tStop all cluster services.
  restart\tStop and start all system components.
  status\tReport on Kubernetes components and system services in detail.
  nodes \tDisplay detailed usage and status for cluster nodes.
  help  \tShow this message.
"
}

init

$*

exit 0

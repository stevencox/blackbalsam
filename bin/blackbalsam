#!/bin/bash

#############################################################
##
##  B L A C K B A L S A M
## 
##    A multi modal data science environment featuring
##       User Experience: Jupyter Notebooks...
##       Compute: CPU / GPU / Apache Spark
##       Storage: NFS / S3 / Alluxio
## 
##
#############################################################
#set -x
set -e

# https://oak-tree.tech/

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
BLACKBALSAM_HOME=$( dirname $DIR )

export RELEASE=blackbalsam
export NAMESPACE=blackbalsam
export VERSION=0.8.2
export DIST=$BLACKBALSAM_HOME/bin
export ETC=$BLACKBALSAM_HOME/etc
export PATH=$DIST/darwin-amd64:$PATH
export helm_dist=helm-v2.16.3-darwin-amd64.tar.gz
export helm_url=https://get.helm.sh/$helm_dist
#172.25.16.225-172.25.16.229

#############################################################
##
##  Make namespace: Initialize the namespace.
##
#############################################################
make_namespace () {
    if [ "$(kubectl get namespaces | grep -c $NAMESPACE)" == 0 ]; then
        kubectl create namespace $NAMESPACE
    fi
}
spark () {
    up () {
	pushd $BLACKBALSAM_HOME/spark-alluxio
	#./build build
	popd
    }
    down () {
	echo
    }
    $*
}
yaml_set_val () {
    yaml-set \
	--change="$1" \
        --value=$2 \
        -F dquote \
        $3
}
#############################################################
##
##  Manage JupyterHub.
##
#############################################################
hub () {
    conf () {
        mkdir -p $DIST
        wget --timestamping --quiet --directory-prefix=$DIST $helm_url
        if [ ! -d $DIST/darwin-amd64 ]; then
            pushd $DIST
            tar xvzf $helm_dist
            popd
        fi
        if [ "$(kubectl get serviceaccount -n kube-system tiller -o yaml | grep -c tiller)" == 0 ]; then
            kubectl --namespace kube-system create serviceaccount tiller
            kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
            helm init --service-account tiller --history-max 100 --wait
            kubectl patch \
		    deployment \
		    tiller-deploy \
                    --namespace=kube-system --type=json \
                    --patch='[{"op": "add", "path": "/spec/template/spec/containers/0/command", "value": ["/tiller", "--listen=localhost:44134"]}]'
            helm version
            helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
            helm repo update
        fi
    }
    jupyterhub_ready () {
        kubectl get deployments \
                --namespace=$NAMESPACE \
                --selector app=jupyterhub,component=hub \
                -o=jsonpath="{.items[?(@.metadata.labels.component=='hub')].status.readyReplicas}"
    }
    wait () {
        while [[ ! "$(jupyterhub_ready)" == 1 ]]; do
            echo waiting
        done
    }
    up () {
	echo installing jupyterhub
        conf
        helm version
	pvc
        cp $ETC/hub/config.yaml.template config.yaml
	yaml_set_val auth.github.clientId $github_client_id config.yaml
	yaml_set_val auth.github.clientSecret $github_client_secret config.yaml
	yaml_set_val auth.github.callbackUrl $github_oauth_callback config.yaml
	yaml_set_val proxy.secretToken $jupyterhub_secret_token config.yaml
	yaml_set_val hub.baseUrl $jupyterhub_baseUrl config.yaml
	
        helm upgrade --install $RELEASE jupyterhub/jupyterhub \
             --namespace $NAMESPACE  \
             --version=$VERSION \
             --values config.yaml
        proxy
        patch
    }
    pvc () {
	echo check for persistent volume claim...
	set +e
	pvc_exists=$(kubectl --namespace=$NAMESPACE get pvc | grep -c jhub-nfs-pvc)
	if [ "$pvc_exists" == 0 ]; then
	    echo install persistent volume claim...
	    kubectl --namespace=$NAMESPACE apply -f $ETC/pvc/
	fi
	set -e
    }	
    proxy () {
	if [ "$(kubectl get mapping | grep -c jupyter)" -eq 0 ]; then
            kubectl apply \
                    -f $ETC/ambassador/jupyterhub-ambassador-mapping.yaml \
                    --namespace=$NAMESPACE
	fi
    }
    patch () {
        # https://github.com/jupyterhub/kubespawner/issues/354        
        kubectl patch deploy \
                --namespace $NAMESPACE hub \
                --type json \
                --patch '[{"op": "replace", "path": "/spec/template/spec/containers/0/command", "value": ["bash", "-c", "\nmkdir -p ~/hotfix\ncp -r /usr/local/lib/python3.6/dist-packages/kubespawner ~/hotfix\nls -R ~/hotfix\npatch ~/hotfix/kubespawner/spawner.py << EOT\n72c72\n<             key=lambda x: x.last_timestamp,\n---\n>             key=lambda x: x.last_timestamp and x.last_timestamp.timestamp() or 0.,\nEOT\n\nPYTHONPATH=$HOME/hotfix jupyterhub --config /srv/jupyterhub_config.py --upgrade-db\n"]}]'

    }
    down () {
	echo uninstalling jupyterhub
        helm version
        if [[ "$(helm ls | grep -v NAME | grep -c jupyterhub)" -gt 0 ]]; then
            echo deleting helm release $RELEASE
            helm delete $RELEASE --purge
        fi
	#kubectl --namespace=$NAMESPACE delete -f $ETC/pvc/
    }
    restart () {
        down
        up
    }
    $*
}
#############################################################
##
##  Manage the software defined reverse proxy.
##
#############################################################
proxy () {
    up () {
	echo installing ambassador...
        helm repo add datawire https://www.getambassador.io
        helm install --name ambassador --namespace $NAMESPACE --set service.loadBalancerIP=$public_ip datawire/ambassador 
    }
    down () {
	echo uninstalling ambassador...
        if [[ "$(helm ls | grep -v NAME | grep -c ambassador)" == 1 ]]; then
            helm delete ambassador --purge
        fi
        for crd in $(kubectl get customresourcedefinitions | grep ambassador | awk '{ print $1  }'); do
	    echo delelting ambassador customresourcedefinitions...
            kubectl delete customresourcedefinition $crd
        done
    }
    dashboard () {
	wget --timestamping --quiet https://metriton.datawire.io/downloads/darwin/edgectl
	chmod +x edgectl
        endpoint_ip=$(kubectl \
                          get svc -n $NAMESPACE \
                          -o=jsonpath="{.items[?(@.metadata.name=='ambassador')].status.loadBalancer.ingress[0].ip}")
        ./edgectl login --namespace=$NAMESPACE $endpoint_ip
    }
    $*
}
#############################################################
##
##  Manage the S3 object store.
##
#############################################################
minio () {
    up () {
	echo installing minio...
        if [ "$(helm list | grep -c minio)" == 1 ]; then
            echo == WARNING ==: A minio release is already installed.
        else
            helm install \
                 --set accessKey=$minio_access_key \
                 --set secretKey=$minio_secret_key \
		 --set persistence.existingClaim=blackbalsam-jhub-nfs-pvc \
		 --set persistence.subPath=minio \
                 --name minio \
                 --namespace $NAMESPACE \
                 stable/minio
        fi
    }
    down () {
	echo uninstalling minio...
        if [[ "$(helm ls | grep -v NAME | awk '{ print $1 }' | grep -c minio)" == 1 ]]; then
            helm delete --purge minio
        fi
    }
    $*
}
#############################################################
##
##  Manage the memory cache data fabric.
##
#############################################################
alluxio () {
    up () {
	echo installing alluxio...
        echo "Configuring and starting alluxio..."
        export ALLUXIO_MASTER_HOST=alluxio-master-0
        export ALLUXIO_WORKER_HOST=alluxio-worker
        export MINIO_BUCKET=covid-19
        export MINIO_HOST=minio
        export MINIO_PORT=9000
        export MINIO_ACCESS_KEY=$minio_access_key
        export MINIO_SECRET_KEY=$minio_secret_key
        export alluxio_conf="alluxio-k8s/singleMaster-localJournal/alluxio-configmap.yaml"
        envsubst < ${alluxio_conf}.template > $alluxio_conf
        echo "  configured. creating kubernetes artifacts."
        kubectl apply \
		--namespace $NAMESPACE \
		--recursive=true \
		-f alluxio-k8s
    }
    down () {
        echo "uninstalling alluxio..."
        kubectl delete \
		--ignore-not-found=true \
		--namespace $NAMESPACE \
		--recursive=true \
		-f alluxio-k8s
    }
    restart () {
        down
        up
    }
    $*
}
#############################################################
##
##  Manage the system as a whole.
##
#############################################################
SYSTEMS="minio alluxio spark proxy hub"
SYSTEMS="minio spark proxy hub"
up () {
    echo installing blackbalsam...
    for system in $SYSTEMS; do
	echo "  --installing $system"
        $system up
    done
}
down () {
    echo uninstalling blackbalsam...
    reversed="$(echo $SYSTEMS | awk '{ for (i=NF; i>1; i--) printf("%s ",$i); print $1; }')"
    for system in $reversed; do
        $system down
    done
    kubectl delete --ignore-not-found=true namespace $NAMESPACE
}
restart () {
    echo "restarting blackbalsam"
    echo "   bringing the system down..."
    down
    echo "   bringing the system up..."
    up
}
status () {
    for kind in deployment pod service pvc pv; do
        message=$( echo $kind | awk '{ print toupper($0) }' )
        echo $message
        kubectl get -n $NAMESPACE $kind 2>&1 | sed "s,^,   ,g"
    done
}
nodes () {
    kubectl describe nodes
}
#############################################################
##
##  Initialize.
##
#############################################################
init () {
    # Require an init file.
    if [ ! -f $HOME/.blackbalsam ]; then
        echo $HOME/.blackbalsam must exist and contain a variable called jupyterhub_secret_token
        exit 1
    fi
    source $HOME/.blackbalsam

    tiller_status=$(kubectl \
			get deployment -n kube-system \
			-o=jsonpath="{.items[?(@.metadata.labels.name=='tiller')].metadata.labels.name}" | \
			grep -c tiller)

    if [ $tiller_status -eq 1 ]; then
	echo tiller already installed. continuing...
	return 
    fi
    echo installing tiller...
    mkdir -p $DIST

    wget --timestamping --quiet --directory-prefix=$DIST $helm_url
    if [ ! -d $DIST/darwin-amd64 ]; then
        pushd $DIST
        tar xvzf $helm_dist
        popd
    fi
    if [ "$(kubectl get serviceaccounts -n kube-system | grep -c tiller)" == 0 ]; then
        kubectl --namespace kube-system create serviceaccount tiller
    fi
    if [ "$(kubectl get clusterrolebinding | grep -c tiller)" == 0 ]; then
        kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
    fi
    helm init --service-account tiller --history-max 100 --wait
    kubectl patch deployment tiller-deploy \
            --namespace=kube-system --type=json \
            --patch='[{"op": "add", "path": "/spec/template/spec/containers/0/command", "value": ["/tiller", "--listen=localhost:44134"]}]'

    helm version
    helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
    helm repo update
}

#############################################################
##
##  Help.
##
#############################################################
help () {
    help_message="\
$0 is a data science cluster architecture.

User Experience Services:
  hub   \tConfigure, install, and uninstall JupyterHub notebook server.

Compute Services:
  spark \tBuild, publish, Apache Spark and Alluxio containers.

Storage Services:
  alluxio\tManage cluster deployment of Alluxio services.
  minio \tManage cluster deployment of the Minio S3 system.

Proxy Services:
  proxy \tManage the programmable Ambassador edge proxy.

Overall Management:
  status\tReport on Kubernetes components of the system in detail.
  up    \tExecute configurations and start all services.
  down  \tStop all cluster components and services
  nodes \tDisplay detailed usage and status for cluster nodes.

Other Commands:
  help  \tShow this message.

"
    printf "$help_message"
}

init

$*

exit 0
